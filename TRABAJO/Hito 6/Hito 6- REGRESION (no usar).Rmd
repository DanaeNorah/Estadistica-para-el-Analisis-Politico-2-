

HITO5 - GRUPO 2 - ANALISIS FACTORIAL EXPLORATORIO SIN VARIABLE DESEMPLEO

```{r}
library(rio)
link='https://github.com/DanaeNorah/Estadistica-para-el-Analisis-Politico-2-/raw/master/TRABAJO/Base%20Final/basefinal.csv'
data = import(link)
summary(data)
row.names(data)=data$Pais
data=na.omit(data)
```

PASO 0: ALISTAR BASE DE DATOS

###variable MLAutonomia
```{r}
library(car)
data$MLAutonomia <- recode(data$MLAutonomia, "0=1 ; 0.25=0.75 ; 0.5=0.5 ; 0.75=0.25 ; 1=0")

data$MLAutonomia <-as.factor(data$MLAutonomia)
data$MLAutonomia=factor(data$MLAutonomia,
                            levels = levels(data$MLAutonomia),
                            labels = c("5","4","3","2","1"),
                            ordered = T)
data$MLAutonomia <-as.numeric(data$MLAutonomia)
table(data$MLAutonomia)
```

###Variable MLViolencia
```{r}
#Le asignamos etiquetas a los valores
data$MLViolencia <- recode(data$MLViolencia,"0.25=0.75 ; 0.5=0.5 ; 0.75=0.25")
data$MLViolencia <-as.factor(data$MLViolencia)
data$MLViolencia=factor(data$MLViolencia,
                            levels = levels(data$MLViolencia),
                            labels = c("3","2","1"),
                            ordered = T)
data$MLViolencia <-as.numeric(data$MLViolencia)
table(data$MLViolencia)
```

###Creo una base de datos con las variables que usare
```{r}
subdata= data[,c(3,4,5,6,7,8,10)]
```

###Preparamos la subdata para la factorizacion
```{r}
subdata$LibertadMov= 100 - subdata$LibertadMov
summary(subdata$LibertadMov)
subdata$ConfianzaSJ= 100 - subdata$ConfianzaSJ
```

###Resumen de data
```{r}
str(subdata)
```
PASO 1: Calculemos matriz de correlación:

```{r}
# esta es:
library(polycor)
corMatrix=polycor::hetcor(subdata)$correlations
```
2. Explorar correlaciones:
```{r}
#Sin evaluar significancia:
library(ggcorrplot)
ggcorrplot(corMatrix)
```

```{r}
#Evaluando significancia:
ggcorrplot(corMatrix,
          p.mat = cor_pmat(corMatrix),
          insig = "blank")
```
```{r}
#te sirve para redondear a dos decimales, pero si se corresolo "matriz_corr" es para sacar
round(corMatrix, 2)
```

3. Verificar si datos permiten factorizar:
```{r}
library(psych)
psych::KMO(corMatrix)
```
4. Verificar si la matriz de correlaciones es adecuada
    - Aqui hay dos pruebas:

 a. Hnula: La matriz de correlacion es una matriz identidad
```{r}
cortest.bartlett(corMatrix,n=nrow(subdata))$p.value>0.05
```
 b. Hnula: La matriz de correlacion es una matriz singular.
```{r}
library(matrixcalc)
is.singular.matrix(corMatrix)
```

5, Determinar en cuantos factores o variables latentes podríamos redimensionar la subdata:
```{r}
fa.parallel(subdata,fm = 'ML', fa = 'fa')
```


6. Redimensionar a numero menor de factores

Resultado inicial:
```{r}
library(GPArotation)
resfa <- fa(subdata,nfactors = 3,cor = 'mixed',rotate = "varimax",fm="minres")
```

```{r}
print(resfa$loadings)
```
RESPUESTA: 57.4% de kas caracteristicas de las variables,es un problema porque se pierde bastante información. tomar decision si se elimina o no

Resultado mejorado:
```{r}
print(resfa$loadings,cutoff = 0.5)
```
Cuando logramos que cada variable se vaya a un factor, tenemos una estructura simple.

Resultado visual:
```{r}
fa.diagram(resfa)
```
CONCLUSION: los tres factores obtenidos en el análisis factorial no corresponde a las variables latentes propuestas segun la revision de literatura.

7. Evaluando Resultado obtenido:
```{r}
#¿La Raíz del error cuadrático medio corregida está cerca a cero?
resfa$crms
```
```{r}
#¿La Raíz del error cuadrático medio de aproximación es menor a 0.05?
resfa$RMSEA
```
```{r}
#¿El índice de Tucker-Lewis es mayor a 0.9?
resfa$TLI
```
```{r}
#¿Qué variables aportaron mas a los factores?
sort(resfa$communality)
```
```{r}
#¿Qué variables contribuyen a mas de un factor?
sort(resfa$complexity)
```

8.Posibles valores proyectados:

Creamos un data set
```{r}
factoriales=as.data.frame(resfa$scores)
head(factoriales)
```

o incluirlos en nuestro subset original

```{r}
subdata$factor1<- factoriales$MR1
subdata$factor2<- factoriales$MR2
subdata$factor3<- factoriales$MR3

summary(factoriales)
```

9. REINCORPORAMOS AL SUBDATA LAS VARIABLES QUE SE NECESITAN PARA LA REGRESION (PAIS,DESIGUALDADDEGENERO Y
   DESEMPELO MUJ)
```{r}
subdata=cbind(data[,c(1,2,9)],subdata)
```

10. INVIERTIR
```{r}
subdata$DesigualdadGenero=subdata$DesigualdadGenero *100
```


Finalmente, veamos relaciones:
```{r}
library(BBmisc)
subdata$factor1=normalize(subdata$factor1, 
                      method = "range", 
                      margin=2, # by column
                      range = c(0, 100))
subdata$factor2=normalize(subdata$factor2, 
                      method = "range", 
                      margin=2, # by column
                      range = c(0, 100))
subdata$factor3=normalize(subdata$factor3, 
                      method = "range", 
                      margin=2, # by column
                      range = c(0, 100))
```
añadir correlaciones con cada factor independiente

You can see them all here:
```{r}
plot(subdata$factor1,subdata$DesigualdadGenero)
plot(subdata$factor2,subdata$DesigualdadGenero)
plot(subdata$factor3,subdata$DesigualdadGenero)
```


#REGRESIONES
```{r}
hipotesis = formula(DesigualdadGenero ~ factor1)
regresion=lm(hipotesis,data=subdata)
summary(regresion)
```
```{r}
hipotesis1 = formula(DesigualdadGenero ~ factor2)
regresion1=lm(hipotesis1,data=subdata)
summary(regresion1)
```

```{r}
hipotesis2 = formula(DesigualdadGenero ~ factor3)
regresion2=lm(hipotesis2,data=subdata)
summary(regresion2)
```

```{r}
hipotesis3 = formula(DesigualdadGenero ~ factor1 + factor2 + factor3)
regresion3=lm(hipotesis3,data=subdata[,-c(1)])
summary(regresion3)
```

#agregamos desmepleo como variable CONTROL (PONER EN ANEXO)
```{r}
hipotesis4 = formula(DesigualdadGenero ~ factor1 + factor2 + factor3+DesempleoMuj)
regresion4=lm(hipotesis4,data=subdata[,-c(1)])
summary(regresion4)
```

#DIAGNOSTICO DE LA REGRESION

  - Para que se considere que el modelo de regresión elegido es el adecuado, debemos verificar algunos 
    requisitos a posteriori:

1. Linealidad:
   - Se asume relación lineal entre Y y X:
```{r}
# linea roja debe tender a horizontal
plot(regresion3, 1)

```
2. Homocedasticidad

  - Se asume que el error del modelo de regresión no afecta la varianza o dispersión de la estimación (MATH^):
```{r}
# linea roja debe tender a horizontal
plot(regresion3, 3)
```

También podemos utilizar el test de Breusch-Pagan:
```{r}
library(lmtest)
# null: modelo homocedastico
bptest(regresion3)
```

  - La probabilidad de homocedasticidad es alta (p-value menor a 0.08), de el modelo muestra homocedasticidad.

3. Normalidad de los residuos

Los residuos, la diferencia entre MATH y MATH^, debe distribuirse de manera normal:
```{r}
# puntos cerca a la diagonal
plot(regresion3, 2)

```

Podemos aplicar el test de Shapiro a los residuos:
```{r}
shapiro.test(regresion3$residuals)
```

4. No multicolinelidad

  - Si los predictores tienen una correlación muy alta entre sí, hay multicolinealidad, lo cual no es deseable:
```{r}
library(DescTools)
VIF(regresion3) # > 5 es problematico
```

5. Valores influyentes
```{r}
plot(regresion3,5)
```
RESPUESTA: Se concluye que si hay casos influyentes que son Nambia, Bolivia

##EL MODELO NOS PERMITE PREDECIR, ESTA ECUACION TIENE COEFICIENTES. SIN EMBARGO, PUEDEN HABER ALGUNOS CASOS QUE INFLUYEN EN EL MODELO QUE HAS COSNTRUIDO PORQUE SON DISTANTES. En este caso el caso 29, dista mucho de los demas casos y altera el calculo de nuestro modelo. 
- no todo valor outlier es influyente pero si todo valor influyente es outlier
```{r}
checkregresion3=as.data.frame(influence.measures(regresion3)$is.inf)
head(checkregresion3)
```